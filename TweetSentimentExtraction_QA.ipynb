{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetSentimentExtraction_QA.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexgastone/tweet_extraction/blob/master/TweetSentimentExtraction_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qfQAtRsMVl7",
        "colab_type": "text"
      },
      "source": [
        "# Tweet Sentiment Extraction as a Question Answering task\n",
        "\n",
        "## The Problem\n",
        "\n",
        "https://www.kaggle.com/c/tweet-sentiment-extraction\n",
        "\n",
        "> \"My ridiculous dog is amazing.\" `sentiment: positive`\n",
        "\n",
        "With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? Pick out the part of the tweet (word or phrase) that reflects the sentiment.\n",
        "\n",
        "> What words in tweets support a positive, negative, or neutral sentiment? \n",
        "\n",
        "## Data\n",
        "In this competition Kaggle has extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with **existing sentiment labels**, used here under creative commons attribution 4.0. international licence. The objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.\n",
        "\n",
        "*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*\n",
        "\n",
        "The supervised training data therefore consists of prelabelled (i.e. sentiment) tweets with their corresponding selected text that best supports that sentiment.\n",
        "\n",
        "## Question Answering \n",
        "The goal of question answering is to `answer` a `question` given a `context`. We can phrase our problem as a question answering task in the following way:\n",
        "> `question`: sentiment label\n",
        "\n",
        "> `context`: tweet\n",
        "\n",
        "> `answer`: selected text\n",
        "\n",
        "This is one of the nine NLP tasks that transformer models, such as Google's pretrained BERT, have been able to solve. Currently, one of the top scoring BERT models for question answering tasks on the SQuAD dataset is ALBERT (lite BERT), an ensemble model published in Sept 2019.\n",
        "\n",
        "We will therefore be implementing this model using the transformers library from [Hugging Face](https://huggingface.co/).\n",
        "\n",
        "For more info on ALBERT, check out https://paperswithcode.com/paper/albert-a-lite-bert-for-self-supervised.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBBHbGvQN5vX",
        "colab_type": "text"
      },
      "source": [
        "## 1.0 Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PydfH2WpRXuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers \\\n",
        "&& cd transformers \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRZned-8WJrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install ./transformers\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m37lLD1gS-Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHCuzhPptH0M",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaQGsAiWXcnd",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3oY9W9cS3jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import train\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# read in as dataframe\n",
        "train_df_full = pd.read_csv(io.BytesIO(uploaded['train.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42T4nvYCTDc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# read in as dataframe\n",
        "test_df = pd.read_csv(io.BytesIO(uploaded['test.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVSuR_emTLdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop nan at row index 314\n",
        "train_df_full.dropna(axis=0, inplace=True)\n",
        "print(f'size of train set: {train_df_full.shape[0]}')\n",
        "print(f'size of test set: {test_df.shape[0]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5SVLRQye66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_full.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3bpe6atlWHW",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Split train into evaluation and training sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H20nJ0vppZOH",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.1. Train/test split by fraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDDcUK5ax3EQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_split(df, frac=0.1):\n",
        "  length = df.shape[0]\n",
        "  cutoff = int(length * frac)\n",
        "\n",
        "  # randomize rows\n",
        "  np.random.seed(32)\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  # split\n",
        "  val_df, train_df = df.iloc[:cutoff], df.iloc[cutoff:]\n",
        "\n",
        "  return train_df, val_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cik2jeE_zpep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df, val_df = train_split(train_df_full)\n",
        "print(f'size of train set: {train_df.shape[0]}')\n",
        "print(f'size of evaluation set: {val_df.shape[0]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_ixdmh9K4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1UOy8nlTgpI",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Convert Training and Evaluation Data to SQuAD-like format for use in training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZLSEb7sZjC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check which column corresponds to question, context, answer, and ID\n",
        "for col in train_df:\n",
        "  print(col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ5BxmN_TLkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad(array):\n",
        "  \"\"\"\"\n",
        "  array: dataset to convert, as numpy array   \n",
        "  returns SQuAD-like dictionary\n",
        "  \"\"\"\n",
        "  output = {} \n",
        "\n",
        "  # SQuAD dataset additionally contains version key along with the data key\n",
        "  output['version'] = 'v1.0'\n",
        "  output['data'] = []\n",
        "\n",
        "  for idx, row in enumerate(array):\n",
        "    answer_list = []\n",
        "    qu_ans = [] # question and answer\n",
        "    paragraphs = [] # context, question, and answer\n",
        "\n",
        "    q_id, context, answer, question = row[0], row[1], row[2], row[-1]\n",
        "    \n",
        "    # for test set\n",
        "    if answer == question: \n",
        "      if idx==0:\n",
        "        print('Testing dataset')\n",
        "      ans_index=None\n",
        "      answer=None\n",
        "      title='Test'\n",
        "\n",
        "    # for training/eval sets (i.e. answer is provided)\n",
        "    else:\n",
        "      if idx == 0:\n",
        "        print('Training dataset')\n",
        "      ans_index = context.lower().find(answer.lower())\n",
        "      if ans_index==-1:\n",
        "        print('No index found')\n",
        "      answer = answer.lower()\n",
        "      title='Train'\n",
        "\n",
        "    answer_list.append({'answer_start': ans_index, 'text': answer})\n",
        "    qu_ans.append({'question': question, 'id': q_id, 'is_impossible': False, 'answers': answer_list})\n",
        "    paragraphs.append({'context': context.lower(), 'qas': qu_ans})\n",
        "\n",
        "    output['data'].append({'title': title, 'paragraphs': paragraphs})\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5zAc5SaTDfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_squad, val_squad, test_squad = squad(np.array(train_df)), squad(np.array(val_df)), squad(np.array(test_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZnJVPDdX6VY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save as json files\n",
        "directory = 'dataset'\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "filename = 'train.json'\n",
        "with open(os.path.join(directory, filename), 'w') as outfile:\n",
        "  json.dump(train_squad, outfile)\n",
        "filename = 'val.json'\n",
        "with open(os.path.join(directory, filename), 'w') as outfile:\n",
        "  json.dump(val_squad, outfile)\n",
        "filename = 'test.json'\n",
        "with open(os.path.join(directory, filename), 'w') as outfile:\n",
        "  json.dump(test_squad, outfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ87q93GDeeL",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Run training \n",
        "\n",
        "Train the model with the training set and evaluate on evaluation set using Hugging Face transformers library. They offer a `run_squad.py` function to finetune the library models for question answering using our own (SQuAD-like) data. The function also converts the SQuAD examples to features before training and saves the tensorboard events file under the `runs` folder using tensorboardX . If we want to run training with fp16 (mixed precision), make sure apex is installed before. The model will be stored in `model_output` folder.\n",
        "\n",
        "*Approximate running time on GPU provided by Colab for 1 epoch: 51min*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8rmmesCzLzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Eg53t3QXZAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python transformers/examples/run_squad.py \\\n",
        "  --model_type albert \\\n",
        "  --model_name_or_path albert-xxlarge-v1 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file /content/dataset/train.json \\\n",
        "  --predict_file /content/dataset/val.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /content/model_output \\\n",
        "  --save_steps 1000 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --fp16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0-UhOkDqoT",
        "colab_type": "text"
      },
      "source": [
        "If have more capabilities, could eventually try using `albert-xxlarge-v2`: 12 repeating layer, 128 embedding, 4096-hidden, 64-heads, 223M parameters\n",
        "ALBERT xxlarge model with no dropout, additional training data and longer training. Or `albert-xxlarge-v1` (v1 seems to do slightly better on SQuAD than v2 for xxlarge). More info at https://github.com/google-research/ALBERT. Also, still have to run k-fold cross validation, train model for specified fold, and when doing inference take average of start and end logits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3i5KXF88zGU",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Check similarity using Jaccard index \n",
        "The Jaccard index is the final evaluation metric. It's defined as the size of the intersection divided by the size of the union of the two sets of text: $J(A,B) = \\frac{A \\cap B}{A \\cup B}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZIUE2JboXDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKP3oqBQoo2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('model_output/predictions_.json', 'r') as f:\n",
        "  eval_predictions = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGDp9cJ-o7qO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('val.json', 'r') as f:\n",
        "  eval_truth = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdR1MeRj2Rt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_jaccard_score(eval_predictions, eval_truth):\n",
        "  eval_truth_list = [item for topic in eval_truth['data'] for item in topic['paragraphs']] \n",
        "  train_score = {'neutral':[], 'positive':[], 'negative':[], 'total':[]}\n",
        "\n",
        "  for idx in range(len(eval_truth_list)):\n",
        "    q_id = eval_truth_list[idx]['qas'][0]['id']\n",
        "    answer = eval_truth_list[idx]['qas'][0]['answers'][0]['text']\n",
        "    sentiment = eval_truth_list[idx]['qas'][0]['question']\n",
        "\n",
        "    score = jaccard(answer, eval_predictions[q_id])\n",
        "\n",
        "    train_score[sentiment].append(score)\n",
        "    train_score['total'].append(score)\n",
        "\n",
        "  for sentiment in ['neutral', 'positive', 'negative', 'total']:\n",
        "    score = np.array(train_score[sentiment])\n",
        "    print(sentiment + ' - ' + str(len(score)) + ' examples, average score: ' + str(score.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXTy1Sn22rkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_jaccard_score(eval_predictions, eval_truth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wz3ZFSQpS_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "eval_truth_list = [item for topic in eval_truth['data'] for item in topic['paragraphs']] \n",
        "train_score = {'neutral':[], 'positive':[], 'negative':[], 'total':[]}\n",
        "\n",
        "for idx in range(len(eval_truth_list)):\n",
        "  q_id = eval_truth_list[idx]['qas'][0]['id']\n",
        "  answer = eval_truth_list[idx]['qas'][0]['answers'][0]['text']\n",
        "  sentiment = eval_truth_list[idx]['qas'][0]['question']\n",
        "\n",
        "  score = jaccard(answer, eval_predictions[q_id])\n",
        "\n",
        "  train_score[sentiment].append(score)\n",
        "  train_score['total'].append(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqqD5rasomLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sentiment in ['neutral', 'positive', 'negative', 'total']:\n",
        "  score = np.array(train_score[sentiment])\n",
        "  print(sentiment + ' - ' + str(len(score)) + ' examples, average score: ' + str(score.mean()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF3dOVN51z-1",
        "colab_type": "text"
      },
      "source": [
        "## 3.0 Train model with K-Fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0RDx7CVx3uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training(model_folder):\n",
        "  !python transformers/examples/run_squad.py \\\n",
        "  --model_type albert \\\n",
        "  --model_name_or_path albert-base-v2 \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file dataset/train.json \\\n",
        "  --predict_file dataset/val.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir $model_folder \\\n",
        "  --save_steps 1000 \\\n",
        "  --threads 4 \\\n",
        "  --fp16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnaZTgxgovzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=32)\n",
        "kf.get_n_splits(train_df_full) \n",
        "print(kf)\n",
        "\n",
        "fold = 0\n",
        "for train_index, val_index in kf.split(train_df_full):\n",
        "  print(f'Fold: {fold}')\n",
        "  #print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
        "  train_fold = train_df_full.iloc[train_index, :]\n",
        "  val_fold = train_df_full.iloc[val_index, :]\n",
        "\n",
        "  # convert to squad datasets\n",
        "  print()\n",
        "  print('Converting to SQuAD datasets...')\n",
        "  train_squad, val_squad = squad(np.array(train_fold)), squad(np.array(val_fold))\n",
        "\n",
        "  # save as json files\n",
        "  directory = 'dataset'\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "  filename = 'train.json'\n",
        "  with open(os.path.join(directory, filename), 'w') as outfile:\n",
        "    json.dump(train_squad, outfile)\n",
        "  filename = 'val.json'\n",
        "  with open(os.path.join(directory, filename), 'w') as outfile:\n",
        "    json.dump(val_squad, outfile)\n",
        "\n",
        "  # run training and save model\n",
        "  print()\n",
        "  print('Running training and evaluation...')\n",
        "  output_folder = '/content/model'+str(fold)\n",
        "  run_training(output_folder)\n",
        "\n",
        "  # calculate Jaccard score\n",
        "  print()\n",
        "  print('Calculating Jaccard scores...')\n",
        "  prediction_file = output_folder + '/predictions_.json'\n",
        "  with open(prediction_file, 'r') as f:\n",
        "    eval_predictions = json.load(f) \n",
        "  get_jaccard_score(eval_predictions, val_squad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JCNRkQwUD56",
        "colab_type": "text"
      },
      "source": [
        "## 4.0 Setup prediction code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCEhOQZ9G5qf",
        "colab_type": "text"
      },
      "source": [
        "Process the tweet and outputs the features necessary for model inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDUNr9JDzKey",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('test.json', 'r') as f:\n",
        "  test = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MuUbhbzUZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = [item for topic in test['data'] for item in topic['paragraphs']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIhQmtHGOS3C",
        "colab_type": "text"
      },
      "source": [
        "For this next part, a lot of it is pulled from https://github.com/spark-ming/albert-qa-demo/, great demo. Retrofits parts of `run_squad.py` and makes useother `squad_metrics` functions for setting up the model configuration, processing tweets from test set and features necessary for the model, then running predictions, constructing `SquadResult` from logits corresponding to start and end of the answer, and finally saving the predictions to files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8qkWB5XOCQd",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp0Pq9z9Y4S0",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import time\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers import (\n",
        "    AlbertConfig,\n",
        "    AlbertForQuestionAnswering,\n",
        "    AlbertTokenizer,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits\n",
        "\n",
        "model_name_or_path = \"/content/model_output\"\n",
        "output_dir = \"\"\n",
        "\n",
        "# Config\n",
        "n_best_size = 1 # choose best prediction\n",
        "max_answer_length = 192 #30\n",
        "do_lower_case = True\n",
        "null_score_diff_threshold = 0.0\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "# Setup model\n",
        "config_class, model_class, tokenizer_class = (\n",
        "    AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)\n",
        "config = config_class.from_pretrained(model_name_or_path)\n",
        "tokenizer = tokenizer_class.from_pretrained(\n",
        "    model_name_or_path, do_lower_case=do_lower_case)\n",
        "model = model_class.from_pretrained(model_name_or_path, config=config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "processor = SquadV2Processor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW52q8XDOKNf",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Get predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVXBsZJtMELN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_prediction(test_data):\n",
        "    \"\"\"Setup test data\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for i, entry in enumerate(test_data):\n",
        "        example = SquadExample(\n",
        "            qas_id=entry['qas'][0]['id'],\n",
        "            question_text=entry['qas'][0]['question'],\n",
        "            context_text=entry['context'],\n",
        "            answer_text=None,\n",
        "            start_position_character=None,\n",
        "            title=\"Predict\",\n",
        "            is_impossible=False,\n",
        "            answers=None,\n",
        "        )\n",
        "\n",
        "        examples.append(example)\n",
        "\n",
        "    features, dataset = squad_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=384,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        return_dataset=\"pt\",\n",
        "        threads=1,\n",
        "    )\n",
        "\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n",
        "    all_results = []\n",
        "    \n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "            example_indices = batch[3]\n",
        "            outputs = model(**inputs)\n",
        "            for i, example_index in enumerate(example_indices):\n",
        "                eval_feature = features[example_index.item()]\n",
        "                unique_id = int(eval_feature.unique_id)\n",
        "                output = [to_list(output[i]) for output in outputs]\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "                all_results.append(result)\n",
        "\n",
        "    output_prediction_file = \"predictions.json\"\n",
        "    output_nbest_file = \"nbest_predictions.json\"\n",
        "    output_null_log_odds_file = \"null_predictions.json\"\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        n_best_size,\n",
        "        max_answer_length,\n",
        "        do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        False,  # verbose_logging\n",
        "        True,  # version_2_with_negative\n",
        "        null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIQOB8vhpcKs",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-sUrcA5nXTH",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "predictions = run_prediction(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M8lMNcE2Zyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('predictions.json', 'r') as f:\n",
        "  test_predictions = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2NdJQlJ1SZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_items = test_predictions.items()\n",
        "test_list = list(test_items)\n",
        "\n",
        "test_df = pd.DataFrame(test_list, columns=(['textID', 'selected_text']))\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}